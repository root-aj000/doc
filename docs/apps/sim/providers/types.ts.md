This TypeScript file acts as a central **definition hub** for interacting with various Large Language Models (LLMs) and their associated services. It defines the standardized types and interfaces required to send requests to different AI providers, receive their responses, manage tool usage, track costs, and handle streaming.

Think of it as the "API contract" for any component in your application that needs to talk to an LLM. It ensures consistency and interoperability, regardless of whether you're using OpenAI, Anthropic, Google, or another provider.

---

## Detailed Explanation

Let's break down each part of the code:

### 1. External Import

```typescript
import type { StreamingExecution } from '@/executor/types'
```

*   **`import type { StreamingExecution } from '@/executor/types'`**: This line imports a type named `StreamingExecution` from another file within the project. The `type` keyword before `import` indicates that `StreamingExecution` is only used for type-checking and will not result in any JavaScript code at runtime. It's likely related to how streaming responses are handled internally by an "executor" component, allowing for a standardized way to manage the lifecycle of a streaming interaction.

### 2. Provider Identification

```typescript
export type ProviderId =
  | 'openai'
  | 'azure-openai'
  | 'anthropic'
  | 'google'
  | 'deepseek'
  | 'xai'
  | 'cerebras'
  | 'groq'
  | 'mistral'
  | 'ollama'
  | 'openrouter'
```

*   **`export type ProviderId = ...`**: This defines a `type` called `ProviderId`. It's a **union type**, meaning a variable of type `ProviderId` can only hold one of the specified string literal values.
*   **Purpose**: This list provides a standardized set of identifiers for the different LLM providers that the system supports. This makes it easy to refer to a specific provider without relying on arbitrary strings.

### 3. Model Pricing Structure

```typescript
/**
 * Model pricing information per million tokens
 */
export interface ModelPricing {
  input: number // Cost per million tokens for input
  cachedInput?: number // Cost per million tokens for cached input (optional)
  output: number // Cost per million tokens for output
  updatedAt: string // ISO timestamp when pricing was last updated
}
```

*   **`export interface ModelPricing`**: This defines an `interface` named `ModelPricing`. Interfaces are blueprints that describe the shape of an object.
*   **`input: number`**: Represents the cost (e.g., in USD) for one million input tokens processed by the model.
*   **`cachedInput?: number`**: An optional property (`?` makes it optional). This could represent a reduced cost for input tokens that are retrieved from a cache rather than being processed from scratch.
*   **`output: number`**: Represents the cost for one million output (completion) tokens generated by the model.
*   **`updatedAt: string`**: A string containing an [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) timestamp, indicating when this pricing information was last updated.
*   **Purpose**: This interface standardizes how pricing data for different models is stored, allowing the application to calculate the cost of LLM interactions.

```typescript
/**
 * Map of model IDs to their pricing information
 */
export type ModelPricingMap = Record<string, ModelPricing>
```

*   **`export type ModelPricingMap = Record<string, ModelPricing>`**: This defines a `type` named `ModelPricingMap`. It uses the TypeScript utility type `Record<K, V>`, which constructs an object type whose property keys are `K` and whose property values are `V`.
*   **Purpose**: This type represents an object where each key is a string (expected to be a model ID, like `'gpt-4'`) and its corresponding value is a `ModelPricing` object. It allows you to easily look up pricing for any given model ID.

### 4. Token Usage Information

```typescript
export interface TokenInfo {
  prompt?: number
  completion?: number
  total?: number
}
```

*   **`export interface TokenInfo`**: Defines an interface for storing information about token usage. All properties are optional (`?`).
*   **`prompt?: number`**: The number of tokens in the input prompt (what you send to the LLM).
*   **`completion?: number`**: The number of tokens in the generated response (what the LLM sends back).
*   **`total?: number`**: The sum of prompt and completion tokens.
*   **Purpose**: Provides a standardized way to report token usage, which is crucial for understanding cost and model performance.

```typescript
export interface TransformedResponse {
  content: string
  tokens?: TokenInfo
}
```

*   **`export interface TransformedResponse`**: Defines an interface for a potentially transformed or simplified response.
*   **`content: string`**: The main text content of the response.
*   **`tokens?: TokenInfo`**: Optional token usage information, conforming to the `TokenInfo` interface.
*   **Purpose**: This might be used internally to represent a stripped-down version of a `ProviderResponse` (defined later), focusing just on the content and token counts.

### 5. Provider Configuration

```typescript
export interface ProviderConfig {
  id: string
  name: string
  description: string
  version: string
  models: string[]
  defaultModel: string
  initialize?: () => Promise<void>
  executeRequest: (
    request: ProviderRequest
  ) => Promise<ProviderResponse | ReadableStream<any> | StreamingExecution>
}
```

*   **`export interface ProviderConfig`**: This is a key interface that defines how each individual LLM provider is configured and how it integrates into the system.
*   **`id: string`**: A unique identifier for the provider (e.g., `'openai'`, matching `ProviderId`).
*   **`name: string`**: A human-readable name for the provider (e.g., `'OpenAI'`).
*   **`description: string`**: A brief description of the provider.
*   **`version: string`**: The version of the provider integration or API it uses.
*   **`models: string[]`**: An array of strings, listing all the model IDs supported by this provider (e.g., `['gpt-3.5-turbo', 'gpt-4']`).
*   **`defaultModel: string`**: The default model to use if one isn't explicitly specified in a request.
*   **`initialize?: () => Promise<void>`**: An optional function that can be called to perform any setup or initialization tasks for the provider (e.g., connecting to an API, loading configurations). It returns a Promise that resolves when initialization is complete.
*   **`executeRequest: (request: ProviderRequest) => Promise<ProviderResponse | ReadableStream<any> | StreamingExecution>`**: This is the most critical part of the `ProviderConfig`. It's a function that takes a `ProviderRequest` object (defined later) and returns a `Promise`.
    *   The Promise can resolve to one of three types:
        *   `ProviderResponse`: A complete, non-streaming response object.
        *   `ReadableStream<any>`: A standard Node.js `ReadableStream` for receiving data in chunks, typically used for streaming responses (e.g., token by token).
        *   `StreamingExecution`: A more structured type, likely from the `StreamingExecution` import, specifically designed for managing complex streaming scenarios.
*   **Purpose**: This interface encapsulates all the necessary metadata and the core interaction logic for a specific LLM provider, allowing the system to treat all providers in a uniform way.

### 6. Function Call / Tool Response

```typescript
export interface FunctionCallResponse {
  name: string
  arguments: Record<string, any>
  startTime?: string
  endTime?: string
  duration?: number
  result?: Record<string, any>
  output?: Record<string, any>
  input?: Record<string, any>
}
```

*   **`export interface FunctionCallResponse`**: Defines the structure for tracking the outcome of a function or tool call made by the LLM.
*   **`name: string`**: The name of the function/tool that was called.
*   **`arguments: Record<string, any>`**: An object representing the arguments passed to the function, where keys are argument names (strings) and values can be of any type.
*   **`startTime?: string`**: Optional ISO timestamp when the function call started.
*   **`endTime?: string`**: Optional ISO timestamp when the function call completed.
*   **`duration?: number`**: Optional duration in milliseconds for the function call.
*   **`result?: Record<string, any>`**: Optional, the raw result returned by the function/tool execution.
*   **`output?: Record<string, any>`**: Optional, a processed or specific output from the function.
*   **`input?: Record<string, any>`**: Optional, the input given to the function.
*   **Purpose**: This interface helps in logging and analyzing the execution of functions or external tools invoked by the LLM, providing details about their arguments, timing, and results.

### 7. Timing Segmentation

```typescript
export interface TimeSegment {
  type: 'model' | 'tool'
  name: string
  startTime: number
  endTime: number
  duration: number
}
```

*   **`export interface TimeSegment`**: Defines a detailed segment of time within an LLM interaction.
*   **`type: 'model' | 'tool'`**: Indicates whether this segment represents time spent in model generation or in a tool execution. This is a union type.
*   **`name: string`**: The name of the specific model or tool involved in this segment.
*   **`startTime: number`**: The start time of the segment, likely in milliseconds (e.g., using `performance.now()`).
*   **`endTime: number`**: The end time of the segment, also in milliseconds.
*   **`duration: number`**: The duration of this segment in milliseconds.
*   **Purpose**: Used to provide a granular timeline of operations within a complex LLM request, especially when tools are involved, helping to debug performance bottlenecks.

### 8. Provider Response Structure

```typescript
export interface ProviderResponse {
  content: string
  model: string
  tokens?: {
    prompt?: number
    completion?: number
    total?: number
  }
  toolCalls?: FunctionCallResponse[]
  toolResults?: any[]
  timing?: {
    startTime: string // ISO timestamp when provider execution started
    endTime: string // ISO timestamp when provider execution completed
    duration: number // Total duration in milliseconds
    modelTime?: number // Time spent in model generation (excluding tool calls)
    toolsTime?: number // Time spent in tool calls
    firstResponseTime?: number // Time to first token/response
    iterations?: number // Number of model calls for tool use
    timeSegments?: TimeSegment[] // Detailed timeline of all operations
  }
  cost?: {
    input: number // Cost in USD for input tokens
    output: number // Cost in USD for output tokens
    total: number // Total cost in USD
    pricing: ModelPricing // The pricing used for calculation
  }
}
```

*   **`export interface ProviderResponse`**: This is another fundamental interface, defining the comprehensive structure of a response received from an LLM provider.
*   **`content: string`**: The primary textual content generated by the LLM.
*   **`model: string`**: The ID of the specific model that generated this response.
*   **`tokens?: { ... }`**: Optional object containing token usage details (prompt, completion, total), similar to `TokenInfo`.
*   **`toolCalls?: FunctionCallResponse[]`**: Optional array of `FunctionCallResponse` objects, indicating any tool calls that the LLM decided to make as part of its reasoning process.
*   **`toolResults?: any[]`**: Optional array of results from any tools that were executed. `any[]` suggests the results can vary widely depending on the tool.
*   **`timing?: { ... }`**: Optional object containing detailed timing information for the request's execution:
    *   `startTime: string` / `endTime: string`: ISO timestamps for the overall execution.
    *   `duration: number`: Total duration in milliseconds.
    *   `modelTime?: number`: Time spent specifically within the LLM generating text, excluding time for tools.
    *   `toolsTime?: number`: Time spent executing external tools.
    *   `firstResponseTime?: number`: Time until the very first token or chunk of response was received.
    *   `iterations?: number`: Number of times the LLM was called (e.g., in a tool-use loop).
    *   `timeSegments?: TimeSegment[]`: An array of `TimeSegment` objects providing a granular breakdown of operations.
*   **`cost?: { ... }`**: Optional object containing calculated cost details for the response:
    *   `input: number`: Cost attributed to input tokens in USD.
    *   `output: number`: Cost attributed to output tokens in USD.
    *   `total: number`: Total cost in USD.
    *   `pricing: ModelPricing`: The `ModelPricing` object that was used to calculate these costs.
*   **Purpose**: This comprehensive interface provides all the necessary information from an LLM interaction, including the generated text, model used, token counts, tool interactions, timing metrics, and cost details.

### 9. Tool Usage Control

```typescript
export type ToolUsageControl = 'auto' | 'force' | 'none'
```

*   **`export type ToolUsageControl = ...`**: A union type defining strategies for how the LLM should use available tools.
*   **`'auto'`**: The LLM decides whether to use a tool based on the conversation context.
*   **`'force'`**: The LLM is forced to use a specific tool (or any tool if a specific one isn't named).
*   **`'none'`**: The LLM is explicitly instructed not to use any tools.
*   **Purpose**: Allows the developer to dictate the LLM's behavior regarding tool invocation.

### 10. Provider Tool Configuration

```typescript
export interface ProviderToolConfig {
  id: string
  name: string
  description: string
  params: Record<string, any>
  parameters: {
    type: string
    properties: Record<string, any>
    required: string[]
  }
  usageControl?: ToolUsageControl
}
```

*   **`export interface ProviderToolConfig`**: Defines how an external tool is described to the LLM and the system.
*   **`id: string`**: A unique identifier for the tool.
*   **`name: string`**: The name of the tool, typically used by the LLM to refer to it.
*   **`description: string`**: A description of what the tool does, which helps the LLM decide when to use it.
*   **`params: Record<string, any>`**: An object containing additional parameters for the tool itself, beyond its function signature (e.g., configuration specific to the tool's implementation).
*   **`parameters: { ... }`**: Describes the **input parameters expected by the tool's function**, following a schema format often used by LLM APIs (like OpenAI's function calling):
    *   `type: string`: Typically `'object'`.
    *   `properties: Record<string, any>`: An object where keys are parameter names and values are their schemas (e.g., `{ "location": { "type": "string", "description": "City name" } }`).
    *   `required: string[]`: An array of strings listing the names of parameters that are mandatory.
*   **`usageControl?: ToolUsageControl`**: Optional, overrides the general tool usage control for this specific tool.
*   **Purpose**: This interface is crucial for enabling function/tool calling. It provides the LLM with the necessary metadata and schema to understand available tools and how to call them.

### 11. Message Structure

```typescript
export interface Message {
  role: 'system' | 'user' | 'assistant' | 'function' | 'tool'
  content: string | null
  name?: string
  function_call?: {
    name: string
    arguments: string
  }
  tool_calls?: Array<{
    id: string
    type: 'function'
    function: {
      name: string
      arguments: string
    }
  }>
  tool_call_id?: string
}
```

*   **`export interface Message`**: Defines the standard structure for a single message in a conversation with an LLM, mimicking common chat API formats.
*   **`role: 'system' | 'user' | 'assistant' | 'function' | 'tool'`**: A union type indicating who sent the message:
    *   `'system'`: Instructions or context for the AI.
    *   `'user'`: The human user's input.
    *   `'assistant'`: The AI's response.
    *   `'function'`: The result of a function call (old OpenAI format).
    *   `'tool'`: The result of a tool call (newer OpenAI format, more general).
*   **`content: string | null`**: The actual text of the message. Can be `null` if the message's primary purpose is a function/tool call rather than text.
*   **`name?: string`**: Optional, the name of the author of this message (e.g., for `function` or `tool` roles to indicate which function/tool provided the content).
*   **`function_call?: { ... }`**: Optional object for legacy or specific function calling mechanisms.
    *   `name: string`: The name of the function to call.
    *   `arguments: string`: A JSON string representing the arguments to the function.
*   **`tool_calls?: Array<{ ... }>`**: Optional array for the newer, more flexible tool calling mechanism.
    *   `id: string`: A unique ID for this specific tool call.
    *   `type: 'function'`: The type of tool call (currently only `'function'`).
    *   `function: { name: string; arguments: string }`: Similar to `function_call`, defining the specific function and its arguments.
*   **`tool_call_id?: string`**: Optional, used in a `tool` role message to link it back to a specific `tool_call` from a previous assistant message, indicating which tool call this message is providing the result for.
*   **Purpose**: This is the fundamental building block for constructing conversational turns with LLMs, supporting both plain text and advanced function/tool calling patterns.

### 12. Provider Request Structure

```typescript
export interface ProviderRequest {
  model: string
  systemPrompt: string
  context?: string
  tools?: ProviderToolConfig[]
  temperature?: number
  maxTokens?: number
  apiKey: string
  messages?: Message[]
  responseFormat?: {
    name: string
    schema: any
    strict?: boolean
  }
  local_execution?: boolean
  workflowId?: string // Optional workflow ID for authentication context
  workspaceId?: string // Optional workspace ID for MCP tool scoping
  chatId?: string // Optional chat ID for checkpoint context
  userId?: string // Optional user ID for tool execution context
  stream?: boolean
  streamToolCalls?: boolean // Whether to stream tool call responses back to user (default: false)
  environmentVariables?: Record<string, string> // Environment variables for tool execution
  workflowVariables?: Record<string, any> // Workflow variables for <variable.name> resolution
  blockData?: Record<string, any> // Runtime block outputs for <block.field> resolution in custom tools
  blockNameMapping?: Record<string, string> // Mapping of block names to IDs for resolution
  isCopilotRequest?: boolean // Flag to indicate this request is from the copilot system
  // Azure OpenAI specific parameters
  azureEndpoint?: string
  azureApiVersion?: string
  // GPT-5 specific parameters
  reasoningEffort?: string
  verbosity?: string
}
```

*   **`export interface ProviderRequest`**: This is the most extensive interface, defining all the possible parameters that can be sent when making a request to an LLM provider.
*   **Core LLM Parameters:**
    *   **`model: string`**: The ID of the model to use for this request.
    *   **`systemPrompt: string`**: Initial instructions or context given to the LLM (like a persona or rules).
    *   **`context?: string`**: Additional contextual information, separate from `systemPrompt`.
    *   **`tools?: ProviderToolConfig[]`**: An optional array of `ProviderToolConfig` objects, describing the tools available for the LLM to use.
    *   **`temperature?: number`**: Controls the randomness of the LLM's output (higher = more creative/random, lower = more deterministic). Typically a value between 0 and 2.
    *   **`maxTokens?: number`**: The maximum number of tokens the LLM should generate in its response.
    *   **`apiKey: string`**: The API key required to authenticate with the LLM provider.
    *   **`messages?: Message[]`**: An optional array of `Message` objects, representing the conversation history or a prompt in a chat-completion format.
    *   **`responseFormat?: { name: string; schema: any; strict?: boolean; }`**: Optional, specifies the desired format for the LLM's response, often used for structured output (e.g., JSON schema).
        *   `name: string`: Name of the response format (e.g., `'json_object'`).
        *   `schema: any`: The schema to validate the output against.
        *   `strict?: boolean`: If true, the output must strictly adhere to the schema.
*   **Execution Context Parameters (for internal system use):**
    *   **`local_execution?: boolean`**: Flag indicating if the execution should happen locally.
    *   **`workflowId?: string`**: Optional ID of the current workflow, potentially used for authentication or context.
    *   **`workspaceId?: string`**: Optional ID of the workspace, perhaps for scoping tool access.
    *   **`chatId?: string`**: Optional ID of the chat conversation, for checkpointing or history.
    *   **`userId?: string`**: Optional ID of the user, for tool execution context or permissions.
*   **Streaming Parameters:**
    *   **`stream?: boolean`**: If `true`, the response should be streamed back token by token.
    *   **`streamToolCalls?: boolean`**: If `true`, stream tool call responses back to the user (defaulting to `false`).
*   **Tool Execution Environment Variables (for custom tools):**
    *   **`environmentVariables?: Record<string, string>`**: Environment variables to be made available during tool execution.
    *   **`workflowVariables?: Record<string, any>`**: Variables from the workflow context for resolving placeholders like `<variable.name>` in custom tools.
    *   **`blockData?: Record<string, any>`**: Outputs from previous blocks in a workflow, for resolving `<block.field>` in custom tools.
    *   **`blockNameMapping?: Record<string, string>`**: A mapping of block names to their IDs for resolution purposes.
*   **Internal Flags:**
    *   **`isCopilotRequest?: boolean`**: A flag to indicate if this request originates from a "copilot" system, potentially altering behavior.
*   **Provider-Specific Parameters:**
    *   **`azureEndpoint?: string`**: Azure OpenAI specific; the endpoint URL for the Azure API.
    *   **`azureApiVersion?: string`**: Azure OpenAI specific; the API version to use.
    *   **`reasoningEffort?: string`**: GPT-5 specific (or similar future models); influences the LLM's reasoning process.
    *   **`verbosity?: string`**: GPT-5 specific; controls how verbose the LLM's output or internal reasoning steps are.
*   **Purpose**: This single, comprehensive interface allows developers to configure every aspect of an LLM request, from basic model selection and prompt engineering to advanced tool integration, streaming, and context management, while also accommodating provider-specific nuances.

### 13. Provider Registry

```typescript
// Map of provider IDs to their configurations
export const providers: Record<string, ProviderConfig> = {}
```

*   **`export const providers: Record<string, ProviderConfig> = {}`**: This line declares and exports a constant variable named `providers`.
*   **`Record<string, ProviderConfig>`**: This type indicates that `providers` is an object where keys are strings (expected to be `ProviderId` values like `'openai'`) and values are `ProviderConfig` objects.
*   **`= {}`**: It is initialized as an empty object.
*   **Purpose**: This object acts as a global registry or dictionary where different LLM providers can register their `ProviderConfig` implementations. Other parts of the application can then look up and interact with any registered provider by its ID.

---

### Simplified Summary

This file is like a detailed instruction manual and contract for integrating with different AI text generators (LLMs) in a software application.

1.  **Who's Who (`ProviderId`):** It lists all the AI companies (like OpenAI, Google) the system can talk to.
2.  **Price Tags (`ModelPricing`, `ModelPricingMap`):** It defines how to store and look up the cost for using different AI models.
3.  **Chat Messages (`Message`):** It sets the standard format for every piece of the conversation, including user questions, AI answers, and even when the AI wants to use an external tool.
4.  **How to Ask the AI (`ProviderRequest`):** This is a big form you fill out to tell the AI what you want. It includes:
    *   Which AI model to use.
    *   The main instruction for the AI.
    *   The conversation history.
    *   Options for temperature (how creative it should be) and max response length.
    *   API keys for authentication.
    *   Details if the AI should use any external "tools" (like a calculator or a calendar API).
    *   Special instructions for streaming responses or for specific providers (like Azure).
5.  **What the AI Gives Back (`ProviderResponse`):** This is the structured report of what the AI sent back. It contains:
    *   The AI's generated text.
    *   Which model responded.
    *   How many "tokens" (words/parts of words) were used.
    *   Information if it called any tools, their results, and how long everything took.
    *   The calculated cost of the interaction.
6.  **How to Set Up Each AI Provider (`ProviderConfig`):** This is like a mini-manual for each AI company, telling the system:
    *   Its name and description.
    *   Which models it offers.
    *   The crucial `executeRequest` function, which is the actual code that sends your `ProviderRequest` to that specific AI provider and returns a `ProviderResponse` (or a stream).
7.  **Global List of Providers (`providers`):** Finally, there's an empty object (`providers`) where all these `ProviderConfig` objects will be stored, making them easily accessible by their ID throughout the application.

In essence, this file creates a robust and flexible framework for building an application that can seamlessly integrate and switch between various LLM services.